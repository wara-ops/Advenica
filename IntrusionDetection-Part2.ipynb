{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"advenica-logo.svg\" width=\"150\" align=\"right\">\n",
    "\n",
    "# Intrusion detection Part 2 - Wordpress (i.e. Apache) logs\n",
    "\n",
    "_(Continued from Part 1)_\n",
    "\n",
    "3. Locating the WordPress Site:\n",
    "    - A search for the WordPress site leads to the `robots.txt` file, which points to the path `/blogblog/`.\n",
    "    - Using a fuzzing tool called `ffuf`, some common WordPress URIs are found and through those the login page.\n",
    "4. Exploiting WordPress Credentials:\n",
    "    - The previously cracked passwords includes access to the WordPress admin account.\n",
    "    - Finally, with admin rights to the entire WordPress site, there are multiple options for further exploration and exploitation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the incident\n",
    "\n",
    "The WordPress site is using an apache webserver and we continue the investigation by looking for further evidence of the intrusion attempt there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some needful things\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install logparser3\n",
    "!{sys.executable} -m pip install drain3\n",
    "!{sys.executable} -m pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pandas settings\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)          # Show all columns\n",
    "pd.set_option('display.max_colwidth', 1000)         # Set a large max column width\n",
    "pd.set_option('display.colheader_justify', 'left')  # Justify column headers to the left\n",
    "pd.set_option('display.expand_frame_repr', False)   # Prevent line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing of the log file to make it more palatable for Drain.\n",
    "\n",
    "- We rearrange the log entry a bit, putting the timestamp first, followed by client ip (which is probably an interesting feature.)\n",
    "- Apache timestamps look a bit iffy, so we transform them into something pandas understand natively.\n",
    "- Log entries are expected look like this: \n",
    "- After preprocessing, the log will only contain single line entries on the form\n",
    "  - `<timestamp> <clientip> <clientid> <clientuser> <statuscode> <size> <refererip> <refererpage> \"<useragent>\" \"<request>\"`\n",
    "  - `2024-07-08T11:12:56+0100 192.168.56.1 - - 200 3640 - - \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\" \"/blogblog/wp-login.php\"`\n",
    "  - *Note*: `clientid` and `clientuser` are almnost always `-` so can probably be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import pandas as pd\n",
    "log_line_re = re.compile(r'^(?P<clientip>\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}) (?P<clientid>.*) (?P<clientuser>.*) \\[(?P<timestamp>.+)\\] \\\"(?P<method>[A-Za-z]+) (?P<request>.+) HTTP/\\d.\\d\\\" (?P<statuscode>\\d{3}) (?P<size>\\d+) \\\"(?P<refererip>https?://\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}:\\d*)?(?P<refererpage>.+)\\\" \\\"(?P<useragent>.+)\\\"$')\n",
    "\n",
    "def convert_datetime(dt):\n",
    "    # Convert apache's weird date format to something more useful\n",
    "    apache_format = \"%d/%b/%Y:%H:%M:%S %z\"\n",
    "    # Define the output format\n",
    "    pandas_format = \"%Y-%m-%dT%H:%M:%S%z\"\n",
    "    # Parse the datetime string and reformat it\n",
    "    dt = datetime.strptime(dt, apache_format)\n",
    "    return dt.strftime(pandas_format)\n",
    "\n",
    "def parse_raw_log(filename):\n",
    "    log_entries = []\n",
    "    num_lines = 0\n",
    "    failed_lines = 0\n",
    "    parsed_lines = 0\n",
    "    with gzip.open(filename, \"rt\", encoding=\"utf8\") as fp:\n",
    "        for line in fp:\n",
    "            num_lines += 1\n",
    "            if not log_line_re.match(line):\n",
    "                failed_lines += 1\n",
    "                print(f\"failed to parse: {repr(line)}\")\n",
    "            else:\n",
    "                parsed_lines += 1\n",
    "                entry = { k: v if v is not None else '-' for k,v in log_line_re.match(line).groupdict().items()}\n",
    "                entry['timestamp'] = convert_datetime(entry['timestamp'])\n",
    "                log_entries.append(entry)\n",
    "    print(f\"lines: {num_lines}, parsed: {parsed_lines}, failed: {failed_lines}\")\n",
    "    return log_entries\n",
    "\n",
    "log_entries = parse_raw_log('access.log.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"preproc_data\"):\n",
    "    os.mkdir(\"preproc_data\")\n",
    "with open(\"preproc_data/preproc-access.log\", \"wt\", encoding=\"utf8\") as fp:\n",
    "    for entry in log_entries:\n",
    "        fp.write(\"{timestamp} {clientip} {clientid} {clientuser} {statuscode} {size} {refererip} {refererpage} \\\"{useragent}\\\" \\\"{request}\\\"\\n\".format(**entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(log_entries)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['statuscode'] = pd.to_numeric(df['statuscode'], downcast=\"unsigned\")\n",
    "df['size'] = pd.to_numeric(df['size'], downcast=\"unsigned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizations\n",
    "seaborn.countplot(data=df, x='method')\n",
    "plt.title('Distribution of HTTP Methods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig = seaborn.countplot(data=df, x='statuscode')\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=90)\n",
    "plt.title('Distribution of status codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('timestamp', inplace=True)\n",
    "df['hour'] = df.index.hour\n",
    "hourly_patterns = df.groupby(['method', 'hour']).size().unstack().T\n",
    "hourly_patterns.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Methods by Hour of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drain & log parsing\n",
    "\n",
    "After preprocessing, the logs have the following format:\n",
    "\n",
    "```log\n",
    "2024-07-08 11:16:26+0100 192.168.56.3 - - 200 4353 https://192.168.56.13:12380 /blogblog/ \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\" \"/blogblog/?p=21\"\n",
    "```\n",
    "With the following fields:\n",
    "\n",
    "```\n",
    "<Timestamp> <Clientip> - - <Statuscode> <Size> <Refererip> <Refererpage> \"<Useragent>\" \"<Request>\"\n",
    "```\n",
    "(Where request will be the event, i.e. Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logparser.Drain import LogParser\n",
    "\n",
    "input_dir = 'preproc_data/' # The input directory of log file\n",
    "output_dir = 'result/'  # The output directory of parsing results\n",
    "log_file = 'preproc-access.log'\n",
    "# time id command argument\n",
    "log_format = '<Timestamp> <Clientip> - - <Statuscode> <Size> <Refererip> <Refererpage> \"<Useragent>\" \"<Content>\"' # Define log format to split message fields\n",
    "# Regular expression list for optional preprocessing (default: [])\n",
    "regex = [\n",
    "    r'([\\da-fA-F]{8,})', # HEX numbers\n",
    "    r'(\\d{5,})', # 'large' integers\n",
    "    r'(\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3})' # IP numbers\n",
    "]\n",
    "st = 0.5  # Similarity threshold\n",
    "depth = 4  # Depth of all leaf nodes\n",
    "\n",
    "parser = LogParser(log_format, indir=input_dir, outdir=output_dir,  depth=depth, st=st, rex=regex)\n",
    "parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsed log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load structured data\n",
    "df= pd.read_csv('result/preproc-access.log_structured.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "templates = pd.read_csv('result/preproc-access.log_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['EncodedEventId'] = label_encoder.fit_transform(df['EventId'])\n",
    "df['EncodedClientip'] = label_encoder.fit_transform(df['Clientip'])\n",
    "df['EncodedUseragent'] = label_encoder.fit_transform(df['Useragent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs Unsupervised\n",
    "\n",
    "Since we have the labels of the data, we can add those classifiers to the structured data.\n",
    "\n",
    "The classes are:\n",
    "  - `normal` for lines that are not part of an attack.\n",
    "  - `fuzzing` for lines that try to locate important wordpress pages.\n",
    "  - `login` for the line where the hacker downloads the login page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attacker fetching robots.txt and index page of wordpress blog\n",
    "recon = list(range(6139, 6140))\n",
    "# Attacker running fuzzing tool for more recon details\n",
    "fuzzing = list(range(6141, 6274))\n",
    "# Attacker fetching login page\n",
    "login = [6275]\n",
    "\n",
    "def mark_anomalies(row):\n",
    "    if row['LineId'] in recon:\n",
    "        return 'recon'\n",
    "    if row['LineId'] in fuzzing:\n",
    "        return 'fuzzing'\n",
    "    if row['LineId'] in login:\n",
    "        return 'login'\n",
    "    return 'normal'\n",
    "\n",
    "df['Classification'] = df.apply(mark_anomalies, axis=1)\n",
    "df['Anomaly'] = df.apply(lambda row: -1 if row['Classification'] != 'normal' else 1, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation forest, unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ldf = df[['EncodedUseragent', 'EncodedEventId', 'EncodedClientip', 'Statuscode', 'Size']]\n",
    "features = df[['EncodedUseragent', 'EncodedEventId', 'EncodedClientip', 'Statuscode', 'Size']]\n",
    "\n",
    "# Train Isolation Forest model\n",
    "model = IsolationForest(contamination=0.01)\n",
    "ldf['PredictedAnomaly'] = model.fit_predict(features)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(df['Anomaly'], ldf['PredictedAnomaly'], labels=[1, -1])\n",
    "\n",
    "# Calculate performance metrics\n",
    "report = classification_report(df['Anomaly'], ldf['PredictedAnomaly'], labels=[1, -1], target_names=['Normal', 'Anomaly'])\n",
    "precision = precision_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "recall = recall_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "f1 = f1_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each actual class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with counts and percentages\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Counts')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Percentages')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest, supervised\n",
    "\n",
    "Next, we make use of the labels and try out random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = df[['EncodedUseragent', 'EncodedEventId', 'EncodedClientip', 'Statuscode', 'Size']]\n",
    "\n",
    "y = df['Anomaly']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[1, -1])\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Counts')\n",
    "plt.show()\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e., by the number of samples in each actual class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with percentages\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Percentages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The supervised approach is again successful right out of the box. The reason for this is (again, and likely) that there is very little (actually no) overlap between the anomalous and non-anomalous log entries (see below.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = list(set(df[df['Anomaly'] == -1]['EventId']))\n",
    "normies = list(set(df[df['Anomaly'] == 1]['EventId']))\n",
    "\n",
    "mixed = [ x for x in normies if x in anomalies]\n",
    "print(len(mixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
