{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"advenica-logo.svg\" width=\"150\" align=\"right\">\n",
    "\n",
    "# Intrusion detection Part 1 - MySQL logs\n",
    "\n",
    "The scenario:\n",
    "\n",
    "A hacker is trying to gain access to a system through the following steps:\n",
    "\n",
    "1. Open Port Scan:\n",
    "    - A simple open port scan of the system reveals that the MySQL port (3306) is open.\n",
    "2.  Gathering Information with `nmap`:\n",
    "    - Using `nmap` and its handy `--script` flag gives some useful information about the MySQL service running on the system.\n",
    "3. Brute-Forcing MySQL Credentials:\n",
    "    - Suspecting that the MySQL database might contain valuable data, Metasploit's `mysql_login` auxiliary module is used to brute-force the server's credentials through a list of common users and potential passwords. \n",
    "    - The superuser (root) of the database server had a weak password.\n",
    "4. Accessing the MySQL Database:\n",
    "    - Upon logging into the MySQL database, there is information related to a WordPress site, including a table with hashed passwords.\n",
    "    - These weakly hashed passwords are easily cracked, revealing plaintext passwords to the site.\n",
    "\n",
    "_(Continued in part 2)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the incident\n",
    "\n",
    "We start our investigation with examining the logs from the mysql database to see if we can find any evidence of an intrusion attempt there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful python modules\n",
    "\n",
    "!pip -q install  pandas seaborn matplotlib scikit-learn logparser3 drain3 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the modules do and links to documentation \n",
    "\n",
    "- [pandas](https://pandas.pydata.org/docs/) is a data analysis and manipulation tool\n",
    "- [seaborn](https://seaborn.pydata.org/tutorial.html) is a statistical data visualization tool\n",
    "- [matplotlib](https://matplotlib.org/stable/users/index) is a library for creating visualizations\n",
    "- [scikit-learn](https://scikit-learn.org/stable/user_guide.html) is a machine learning and predictive data analysis tool\n",
    "- [logparser3](https://github.com/logpai/logparser) is a machine learning toolkit for automated log parsing\n",
    "- [drain3](https://github.com/logpai/Drain3) is an online log template miner\n",
    "- [numpy](https://numpy.org/doc/stable/) is a package for scientific computing used by the above tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure pandas display options\n",
    "\n",
    "In order to keep output readable we change a couple of default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)          # Show all columns\n",
    "pd.set_option('display.max_colwidth', 1000)         # Set a large max column width\n",
    "pd.set_option('display.colheader_justify', 'left')  # Justify column headers to the left\n",
    "pd.set_option('display.expand_frame_repr', False)   # Prevent line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing of the log file to make it more suitable for our log analyzer (drain).\n",
    "\n",
    "This is a messy step, because it addresses the gap, or impedence mismatch if you want, between the log file format that you have to work with and the log file format that you _wish_ that you had. This step is always going to be present in a real application, but unfortunately will depend on the circumstances, so consider this an example of what it _could_ look like. \n",
    "\n",
    "- We parse the SQL statements & the table operated on since these can both be potential features of interest\n",
    "- Timestamps are expected to look like `2024-07-08T10:27:53.653910Z` (so in UTC)\n",
    "- Log entries are expected look like this: \n",
    "    - `2024-07-10T05:25:05.058758Z49509 Query  SELECT * FROM wp_posts WHERE ID = 47 LIMIT 1`\n",
    "    - *Note 1*: the number after Z in the timestamp is actually a mysql ID number for the transaction.\n",
    "    - *Note 2*: The only command with a space is `Init DB` - in order to make it more manageable, we replace it with `InitDB`.\n",
    "- Log entries can contain newlines so we use the fact that a timestamp marks start of the next entry.\n",
    "    - We use split to remove unnecessary whitespace (tabs, newlines, spaces and multiples thereof)\n",
    "- After preprocessing, the log will only contain single line entries on the form\n",
    "    - `<timestamp> <id> <command> <sql action (if any)> <table (if any>)> <and anything else>`\n",
    "    - `2024-07-08T10:27:53.654594Z 225 Query SELECT wp_posts SELECT * FROM wp_posts WHERE ID = 21 LIMIT 1`\n",
    "- Finally, we store the log entries in a pandas [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) with columns `timestamp`, `id`, `command`, `action`, `table`, and `argument`.\n",
    "    - If any optional part (`action`, `table`, and `argument`) is missing, a `-` is used as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "timestamp_re = re.compile(r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{6}Z)')\n",
    "logline_re = re.compile(r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{6}Z) *(\\d+) (Init DB|\\S+)(?: ?(.*))$')\n",
    "\n",
    "# Regex parsing of SQL statements to get the action\n",
    "update_re = re.compile(r'UPDATE (?:`)(\\S*)(?:`) .*')\n",
    "select_w_table_re = re.compile(r'SELECT\\s+.*\\s*FROM\\s*(\\S+).*')\n",
    "select_wo_table_re = re.compile(r'SELECT\\s+.*')\n",
    "delete_re = re.compile(r'DELETE\\s+FROM\\s*`(\\S+)`.*')\n",
    "show_re = re.compile(r'SHOW.*`(\\S+)`')\n",
    "insert_re = re.compile(r'INSERT INTO\\s+.*`(\\S+)`.*')\n",
    "set_re = re.compile(r'SET .*')\n",
    "flush_re = re.compile(r'FLUSH .*')\n",
    "\n",
    "def get_sql_action(entry):\n",
    "    if select_w_table_re.match(entry):\n",
    "         return \"SELECT\", select_w_table_re.match(entry).group(1)\n",
    "    if select_wo_table_re.match(entry):\n",
    "         return \"SELECT\", \"-\"\n",
    "    if delete_re.match(entry):\n",
    "         return \"DELETE\", delete_re.match(entry).group(1)\n",
    "    if insert_re.match(entry):\n",
    "         return \"INSERT\", insert_re.match(entry).group(1)\n",
    "    if update_re.match(entry):\n",
    "         return \"UPDATE\", update_re.match(entry).group(1)\n",
    "    if set_re.match(entry):\n",
    "         return \"SET\", \"-\"\n",
    "    if show_re.match(entry):\n",
    "         return \"SHOW\", show_re.match(entry).group(1)\n",
    "    if flush_re.match(entry):\n",
    "         return \"FLUSH\", \"-\"\n",
    "    return '-', '-'\n",
    "\n",
    "def parse_log(file_path):\n",
    "    log_entries = []\n",
    "    ofile, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    if not os.path.exists(\"preproc_data\"):\n",
    "        os.mkdir(\"preproc_data\")\n",
    "    with gzip.open(file_path, 'rt', encoding=\"utf8\") as file, \\\n",
    "         open(f\"preproc_data/preproc-{ofile}\", \"wt\", encoding=\"utf8\") as of:\n",
    "        full_entry = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\"/usr/sbin/mysqld\") or \\\n",
    "                line.startswith(\"Tcp port:\") or \\\n",
    "                line.startswith(\"Time    \"):\n",
    "                   print(f\"Skipping: {line.strip()}\")\n",
    "                   continue\n",
    "            if timestamp_re.match(line) and len(full_entry) > 0:\n",
    "               # Split removes unnecessary whitespace\n",
    "               full_entry = \" \".join(full_entry.split())\n",
    "               match = logline_re.match(full_entry)\n",
    "               if match:\n",
    "                    new_entry = list(match.groups())\n",
    "                    arguments = new_entry.pop()\n",
    "                    if new_entry[2] == 'Query':\n",
    "                         new_entry.extend(get_sql_action(arguments))\n",
    "                    elif new_entry[2] == 'Init DB':\n",
    "                         new_entry[2] = 'InitDB' \n",
    "                         new_entry.extend(['-', '-'])\n",
    "                    else:\n",
    "                         new_entry.extend(['-', '-'])\n",
    "                    new_entry.append(arguments if arguments else '-')\n",
    "                    log_entries.append(new_entry)\n",
    "                    of.write(\" \".join([ s for s in new_entry if s]) + \"\\n\")\n",
    "               else:\n",
    "                    print(f\"skipping '{full_entry}'\")\n",
    "               full_entry = line.strip()\n",
    "            else:\n",
    "               full_entry += line\n",
    "    return pd.DataFrame(log_entries, columns=['timestamp', 'id', 'command', 'action', 'table', 'argument'])\n",
    "\n",
    "df = parse_log('raw_data/general.log.gz')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['id'] = pd.to_numeric(df['id'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies\n",
    "\n",
    "For the training data, we happen to know which log id's are part of an anomaly.\n",
    "\n",
    "We will make use of this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label anomalies\n",
    "login_attack = [2523, 2611] + list(range(2630, 2700))\n",
    "successful_login  = [2541, 2700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some data visualisations\n",
    "\n",
    "Just a couple of plots showing what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizations\n",
    "seaborn.countplot(data=df, x='action')\n",
    "plt.title('Distribution of SQL Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig = seaborn.countplot(data=df, x='table')\n",
    "fig.tick_params(\"x\", labelrotation=90)\n",
    "plt.title('Distribution of SQL table used')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIXME: Not idempotent since set_index(, inplace=True) destroys original format\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df['hour'] = df.index.hour\n",
    "hourly_patterns = df.groupby(['action', 'hour']).size().unstack().T\n",
    "hourly_patterns.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('SQL Actions by Hour of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df.index.hour\n",
    "hourly_patterns = df.groupby(['table', 'hour']).size().unstack().T\n",
    "hourly_patterns.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('SQL table by Hour of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_patterns = df.groupby(['action', 'table']).size().reset_index(name='counts')\n",
    "frequent_patterns.sort_values(by='counts', ascending=False, inplace=True)\n",
    "print(frequent_patterns.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = df.groupby('action')['id'].describe(percentiles=[.01, .99])\n",
    "print(thresholds)\n",
    "\n",
    "thresholds = df.groupby('table')['id'].describe(percentiles=[.01, .99])\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "with contextlib.redirect_stdout(None):\n",
    "    print(\"will not print\")\n",
    "\n",
    "print(\"this will print\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drain & log parsing\n",
    "\n",
    "Next, we use drain to parse the (pre-processed) logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logparser.Drain import LogParser\n",
    "\n",
    "input_dir = 'preproc_data/' # The input directory of log file\n",
    "output_dir = 'result/'  # The output directory of parsing results\n",
    "log_file = 'preproc-general.log'\n",
    "# time id command argument\n",
    "log_format = '<Timestamp>Z <Id> <Command> <Action> <Table> <Content>' # Define log format to split message fields\n",
    "# Regular expression list for optional preprocessing (default: [])\n",
    "regex = [\n",
    "    r'([\\da-fA-F]{8,})', # HEX numbers\n",
    "    r'(\\d{5,})', # 'large' integers\n",
    "    r'(\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3})' # IP numbers\n",
    "]\n",
    "st = 0.5  # Similarity threshold\n",
    "depth = 4  # Depth of all leaf nodes\n",
    "\n",
    "parser = LogParser(log_format, indir=input_dir, outdir=output_dir,  depth=depth, st=st, rex=regex)\n",
    "\n",
    "import contextlib\n",
    "\n",
    "with contextlib.redirect_stdout(None):\n",
    "    parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsed log data\n",
    "\n",
    "Now the data is parsed and has a bit of structure, so we can load it into a dataframe and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load structured data\n",
    "dtypes = {\n",
    "    'LineId': 'int64',\n",
    "    'Timestamp': 'str',\n",
    "    'Id': 'int64',\n",
    "    'Command': 'str',\n",
    "    'Action': 'str',\n",
    "    'Table': 'str',\n",
    "    'Content': 'str',\n",
    "    'EventId': 'str',\n",
    "    'EventTemplate': 'str',\n",
    "    'ParameterList': 'str'\n",
    "}\n",
    "df = pd.read_csv('result/preproc-general.log_structured.csv', dtype=dtypes)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], utc=True)\n",
    "templates = pd.read_csv('result/preproc-general.log_templates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs Unsupervised\n",
    "\n",
    "Since we have the labels of the data, we can add those classifiers to the structured data.\n",
    "\n",
    "The classes are:\n",
    "  - `normal` for lines that are not part of an attack.\n",
    "  - `login_attack` for lines that are part of an attempt to brute force database logins.\n",
    "  - `get_user_table` for lines that are part of an attack which downloads the user table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_anomalies(row):\n",
    "    if row['Id'] in login_attack:\n",
    "        return 'login_attack'\n",
    "    if row['Id'] in successful_login:\n",
    "        return 'get_user_table'\n",
    "    return 'normal'\n",
    "\n",
    "df['Classification'] = df.apply(mark_anomalies, axis=1)\n",
    "df['Anomaly'] = df.apply(lambda row: -1 if row['Classification'] != 'normal' else 1, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "When training a model, it's mostly numbers that matter, so we encode the features we think are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['EncodedEventId'] = label_encoder.fit_transform(df['EventId'])\n",
    "df['EncodedCommand'] = label_encoder.fit_transform(df['Command'])\n",
    "df['EncodedAction'] = label_encoder.fit_transform(df['Action'])\n",
    "df['EncodedTable'] = label_encoder.fit_transform(df['Table'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation forest, unsupervised\n",
    "\n",
    "Let's take the preprocessed, parsed & encoded data for a spin with isolation forest.\n",
    "\n",
    "This is not terrible. Right out of the box, it is really good at identifying normal logs and at times not terrible at identifying anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ldf = df[['EncodedEventId', 'EncodedCommand', 'EncodedAction', 'EncodedTable']]\n",
    "features = df[['EncodedEventId', 'EncodedCommand', 'EncodedAction', 'EncodedTable']]\n",
    "\n",
    "# Train Isolation Forest model\n",
    "model = IsolationForest(contamination=0.01)\n",
    "ldf['PredictedAnomaly'] = model.fit_predict(features)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(df['Anomaly'], ldf['PredictedAnomaly'], labels=[1, -1])\n",
    "\n",
    "# Calculate performance metrics\n",
    "report = classification_report(df['Anomaly'], ldf['PredictedAnomaly'], labels=[1, -1], target_names=['Normal', 'Anomaly'])\n",
    "precision = precision_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "recall = recall_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "f1 = f1_score(df['Anomaly'], ldf['PredictedAnomaly'], pos_label=-1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each actual class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with counts and percentages\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Counts')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Percentages')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest, supervised\n",
    "\n",
    "Next, we make use of the labels and try out random forest.\n",
    "\n",
    "This approach is quite successful since there is very little overlap between anomalous and non-anomalous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label anomalies\n",
    "login_attack = [2523, 2611] + list(range(2630, 2700))\n",
    "successful_login  = [ 2541, 2700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = df[['EncodedEventId', 'EncodedCommand', 'EncodedAction', 'EncodedTable']]\n",
    "y = df['Anomaly']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[1, -1])\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Counts')\n",
    "plt.show()\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e., by the number of samples in each actual class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix as a heatmap with percentages\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', xticklabels=['Predicted Normal', 'Predicted Anomaly'], yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix with Percentages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The supervised approach is surprisingly successful without any extra bells and whistles. The reason for this is (probably) that there is very little overlap between the anomalous and non-anomalous log entries as seen below. Only three events occur as both anomalies and non-anomalies; the rest are only found in one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = list(set(df[df['Anomaly'] == -1]['EventId']))\n",
    "normies = list(set(df[df['Anomaly'] == 1]['EventId']))\n",
    "\n",
    "mixed = [ x for x in normies if x in anomalies]\n",
    "print(len(mixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
